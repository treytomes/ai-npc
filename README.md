# ai-npc
**Experiments in building LLM-based NPCs.**

Exactly how large of a language model do you need to run an NPC?

Let's say that we want to populate a generic JRPG town.  There are an expected cast of characters:

* Some number of generate townsfolk that sometimes share game-related story, but often just prattle on about something inconsequential.
* Town guards that often repeat a single stock statement ad-infinitum.
* Shopkeepers.  Items, weapons, armor, etc.
* Some sort of town leader.

The goal of this experiment is to analyze the feasibility of running a local LLM on a mid-grade gaming laptop.

## Small Language Models

I'm going to defer to more of an expert on this matter:
[Small Language Models (SLM): A Comprehensive Overview](https://huggingface.co/blog/jjokah/small-language-model)

	`Small Language Models (SLMs) are lightweight versions of traditional language models designed to operate efficiently on resource-constrained environments such as smartphones, embedded systems, or low-power computers.`

Can we use an SLM to make NPCs just a little bit smarter?

There's a [Llama model](https://huggingface.co/meta-llama/Llama-3.2-1B) with only 1 billion parameters; significantly less than any LLM you'd see today.  The compute power needed to run an model seems to go up quickly as you increase the parameter count, so reducing that as far as is reasonable would be nice.  The quantized edition of this model uses lower-precision math, which might boost the performance even more.  Is it worth it?

The largest caveat here is the licensing.  Llama isn't really open source, and the licensing agreement is full of rules.  Missing a rule can get me in trouble.

[Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B) has half the parameters of the Llama model (and twice the performance??), and it is released under the apache-2.0 license, which I am personally a lot more comfortable with.  Care should still be taken before creating anything commercial with this though.

[Play with Qwen](https://qwen.ai/home)

## Ollama

The ollama server can run the model we need.  A problem here is that, as we are trying to embed this in a game of some kind, we don't want our users to first need to install and configure a server.  How can we streamline this process for them?

[Download Ollama](https://ollama.com/download/linux)

Ollama can be installed from Linux with a single curl command that runs a bash script.

My initial stab at solving this (with lots of AI coding help) will first check to see if a local Ollama is installed.  It will download the Ollama server, install it to a local app storage folder, then run it and download the qwen2.5-0.5b model into itself.  You can review the code in the `features/01-ollama-management` branch to see this at work.

## OllamaSharp

While I *can* write an entire API layer around the OllamaManager, I'd rather not.  It's not the purpose of this experiment.  I'll be testing out the OllamaSharp library:

* [GitHub](https://github.com/awaescher/OllamaSharp)
* [Getting Started](https://deepwiki.com/awaescher/OllamaSharp/2-getting-started)

In the `features/02-ollamasharp` branch I'm bringing in the OllamaSharp library, reimplementing the existing direct calls to Ollama with their OllamaSharp wrappers, and running some chat completions against the Qwen model.  The system is really simple to use.

I'm wrapping up this feature branch with the OllamaRepo, which wraps the process management and OllamaSharp interaction under one hood.  There's more I could do with the Microsoft.Extensions.AI integration, but that would be outside the scope of this experiment.

One problem that might come back to bite us in the future:  If the app catastrophically fails (e.g. the debugger suddenly stopped), the ollama server might be left running.  I might need a bash process that cleans up any lingering ollamas on it's way out.

## Tool Usage

A shop keeper will need to know their inventory, and need to be able to process transactions with the user.
Townsfolk might need to update quest details.
We need to give the LLM a way to access the app's internal systems.

OllamaSharp has a built-in tool system, [https://deepwiki.com/awaescher/OllamaSharp/3.3-tool-support](Tool Support), which I was unfortunately not able to get working correctly.  Attaching the attribute to methods would generate the source code for the tool class, and I'm generating the assembly documentation as detailed in the instructions, but the compiler can't find the generated code.  After debugging for a bit, I'm aiming for a slightly different approach.

You can manually define tools in OllamaSharp if you don't mind processing the tool responses manually.  I **do** mind, so instead I've grabbed a copy of some of the generated source code so we can generate *that* manually:

```cs
/// <summary>
/// This class was auto-generated by the OllamaSharp ToolSourceGenerator.
/// </summary>
public class GetWeatherTool : OllamaSharp.Models.Chat.Tool, OllamaSharp.Tools.IInvokableTool
{
	/// <summary>
	/// Initializes a new instance with metadata about the original method.
	/// </summary>
	public GetWeatherTool()
	{
		this.Function = new OllamaSharp.Models.Chat.Function
		{
			Name = "GetWeather",
			Description = "Gets the current weather for a given location."
		};

		this.Function.Parameters = new OllamaSharp.Models.Chat.Parameters
		{
			Properties = new Dictionary<string, OllamaSharp.Models.Chat.Property>
				{
					{ "location", new OllamaSharp.Models.Chat.Property { Type = "string", Description = "The location or city to get the weather for" } },
					{ "unit", new OllamaSharp.Models.Chat.Property { Type = "string", Description = "The unit to measure the temperature in", Enum = new[] {"Celsius", "Fahrenheit"} } }
				},
			Required = new[] { "location", "unit" }
		};
	}

	/// <summary>
	/// Invokes the tool with given arguments synchronously
	/// </summary>
	/// <param name="args">The arguments to invoke the tool with</param>
	/// <returns>The result of the invoked tool</returns>
	public object? InvokeMethod(IDictionary<string, object?>? args)
	{
		if (args == null) args = new Dictionary<string, object?>();
		string location = (string?)args["location"] ?? "";
		Unit unit = (Unit)Enum.Parse(typeof(Unit), args["unit"]?.ToString() ?? "", ignoreCase: true);

		var result = GetWeather(location, unit);
		return result;
	}

	public static string GetWeather(string location, Unit unit)
	{
		return unit switch
		{
			Unit.Fahrenheit => $"It's cold at only 3.14159° {unit} in {location}.",
			Unit.Celsius => $"It's warm at only 23 degrees {unit} in {location}, but is {location} a real place?",
			_ => "I don't really know.",
		};
	}
}
```

This allowed me to successfully inject the tool into the chat, but Qwen was stubbornly refusing to use it.  I found this post after a bit of digging:

[Tool Call Issues with Qwen2.5-VL Models (7B & 72B) under vLLM](https://github.com/QwenLM/Qwen3-VL/issues/1093)

Qwen3 seems to require a custom system prompt in order to properly use tools.  I built my own system prompt based on these notes with pretty good results:

```
You are a helpful assistant.

You respond honestly, prioritizing details in order from:
1. tools you have access to,
2. the current conversation,
3. then any pre-existing knowledge on the subject.

Do not make anything up.  If you do not know something, let the user know.
Don't attempt math or numerical conversions without a tool.
No need to be overly verbose in your response unless prompted to do so.

If the user appears to be asking for the weather do not make up a response. Use the GetWeather tool to get the answer.
```

Qwen needs a lot of prompting to get it to not make things up.  Even with this system prompt, we have a lot of work to do:

```
AINPC Ollama Test
----------------------------------
Ollama server is running.
Beginning interactive chat.
You: What is the weather in Dallas?
Assistant: The weather in Dallas is expected to be very cold with a high of -17.86°F at only 3.14159 degrees Fahrenheit on the Fahrenheit scale.
You: 
Server stopped.
```

A custom base class for tools (BaseOllamaTool) helps with the boilerplate.  It's re-inventing the wheel a bit, but my wheel actually works for me.

At this point, we *might* have everything we need to start building out some actors.
