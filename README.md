# ai-npc
**Experiments in building LLM-based NPCs.**

Exactly how large of a language model do you need to run an NPC?

Let's say that we want to populate a generic JRPG town.  There are an expected cast of characters:

* Some number of generate townsfolk that sometimes share game-related story, but often just prattle on about something inconsequential.
* Town guards that often repeat a single stock statement ad-infinitum.
* Shopkeepers.  Items, weapons, armor, etc.
* Some sort of town leader.

The goal of this experiment is to analyze the feasibility of running a local LLM on a mid-grade gaming laptop.

## Small Language Models

I'm going to defer to more of an expert on this matter:
[Small Language Models (SLM): A Comprehensive Overview](https://huggingface.co/blog/jjokah/small-language-model)

	`Small Language Models (SLMs) are lightweight versions of traditional language models designed to operate efficiently on resource-constrained environments such as smartphones, embedded systems, or low-power computers.`

Can we use an SLM to make NPCs just a little bit smarter?

There's a [Llama model](https://huggingface.co/meta-llama/Llama-3.2-1B) with only 1 billion parameters; significantly less than any LLM you'd see today.  The compute power needed to run an model seems to go up quickly as you increase the parameter count, so reducing that as far as is reasonable would be nice.  The quantized edition of this model uses lower-precision math, which might boost the performance even more.  Is it worth it?

The largest caveat here is the licensing.  Llama isn't really open source, and the licensing agreement is full of rules.  Missing a rule can get me in trouble.

[Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B) has half the parameters of the Llama model (and twice the performance??), and it is released under the apache-2.0 license, which I am personally a lot more comfortable with.  Care should still be taken before creating anything commercial with this though.

[Play with Qwen](https://qwen.ai/home)

## Ollama

The ollama server can run the model we need.  A problem here is that, as we are trying to embed this in a game of some kind, we don't want our users to first need to install and configure a server.  How can we streamline this process for them?

[Download Ollama](https://ollama.com/download/linux)

Ollama can be installed from Linux with a single curl command that runs a bash script.

My initial stab at solving this (with lots of AI coding help) will first check to see if a local Ollama is installed.  It will download the Ollama server, install it to a local app storage folder, then run it and download the qwen2.5-0.5b model into itself.  You can review the code in the `features/01-ollama-management` branch to see this at work.

## OllamaSharp

While I *can* write an entire API layer around the OllamaManager, I'd rather not.  It's not the purpose of this experiment.  I'll be testing out the OllamaSharp library:

* [GitHub](https://github.com/awaescher/OllamaSharp)
* [Getting Started](https://deepwiki.com/awaescher/OllamaSharp/2-getting-started)

In the `features/02-ollamasharp` branch I'm bringing in the OllamaSharp library, reimplementing the existing direct calls to Ollama with their OllamaSharp wrappers, and running some chat completions against the Qwen model.  The system is really simple to use.

I'm wrapping up this feature branch with the OllamaRepo, which wraps the process management and OllamaSharp interaction under one hood.  There's more I could do with the Microsoft.Extensions.AI integration, but that would be outside the scope of this experiment.

One problem that might come back to bite us in the future:  If the app catastrophically fails (e.g. the debugger suddenly stopped), the ollama server might be left running.  I might need a bash process that cleans up any lingering ollamas on it's way out.

## Tool Usage

A shop keeper will need to know their inventory, and need to be able to process transactions with the user.
Townsfolk might need to update quest details.
We need to give the LLM a way to access the app's internal systems.

OllamaSharp has a built-in tool system, [https://deepwiki.com/awaescher/OllamaSharp/3.3-tool-support](Tool Support), which I was unfortunately not able to get working correctly.  Attaching the attribute to methods would generate the source code for the tool class, and I'm generating the assembly documentation as detailed in the instructions, but the compiler can't find the generated code.  After debugging for a bit, I'm aiming for a slightly different approach.

You can manually define tools in OllamaSharp if you don't mind processing the tool responses manually.  I **do** mind, so instead I've grabbed a copy of some of the generated source code so we can generate *that* manually:

```cs
/// <summary>
/// This class was auto-generated by the OllamaSharp ToolSourceGenerator.
/// </summary>
public class GetWeatherTool : OllamaSharp.Models.Chat.Tool, OllamaSharp.Tools.IInvokableTool
{
	/// <summary>
	/// Initializes a new instance with metadata about the original method.
	/// </summary>
	public GetWeatherTool()
	{
		this.Function = new OllamaSharp.Models.Chat.Function
		{
			Name = "GetWeather",
			Description = "Gets the current weather for a given location."
		};

		this.Function.Parameters = new OllamaSharp.Models.Chat.Parameters
		{
			Properties = new Dictionary<string, OllamaSharp.Models.Chat.Property>
				{
					{ "location", new OllamaSharp.Models.Chat.Property { Type = "string", Description = "The location or city to get the weather for" } },
					{ "unit", new OllamaSharp.Models.Chat.Property { Type = "string", Description = "The unit to measure the temperature in", Enum = new[] {"Celsius", "Fahrenheit"} } }
				},
			Required = new[] { "location", "unit" }
		};
	}

	/// <summary>
	/// Invokes the tool with given arguments synchronously
	/// </summary>
	/// <param name="args">The arguments to invoke the tool with</param>
	/// <returns>The result of the invoked tool</returns>
	public object? InvokeMethod(IDictionary<string, object?>? args)
	{
		if (args == null) args = new Dictionary<string, object?>();
		string location = (string?)args["location"] ?? "";
		Unit unit = (Unit)Enum.Parse(typeof(Unit), args["unit"]?.ToString() ?? "", ignoreCase: true);

		var result = GetWeather(location, unit);
		return result;
	}

	public static string GetWeather(string location, Unit unit)
	{
		return unit switch
		{
			Unit.Fahrenheit => $"It's cold at only 3.14159° {unit} in {location}.",
			Unit.Celsius => $"It's warm at only 23 degrees {unit} in {location}, but is {location} a real place?",
			_ => "I don't really know.",
		};
	}
}
```

This allowed me to successfully inject the tool into the chat, but Qwen was stubbornly refusing to use it.  I found this post after a bit of digging:

[Tool Call Issues with Qwen2.5-VL Models (7B & 72B) under vLLM](https://github.com/QwenLM/Qwen3-VL/issues/1093)

Qwen3 seems to require a custom system prompt in order to properly use tools.  I built my own system prompt based on these notes with pretty good results:

```
You are a helpful assistant.

You respond honestly, prioritizing details in order from:
1. tools you have access to,
2. the current conversation,
3. then any pre-existing knowledge on the subject.

Do not make anything up.  If you do not know something, let the user know.
Don't attempt math or numerical conversions without a tool.
No need to be overly verbose in your response unless prompted to do so.

If the user appears to be asking for the weather do not make up a response. Use the GetWeather tool to get the answer.
```

Qwen needs a lot of prompting to get it to not make things up.  Even with this system prompt, we have a lot of work to do:

```
AINPC Ollama Test
----------------------------------
Ollama server is running.
Beginning interactive chat.
You: What is the weather in Dallas?
Assistant: The weather in Dallas is expected to be very cold with a high of -17.86°F at only 3.14159 degrees Fahrenheit on the Fahrenheit scale.
You: 
Server stopped.
```

A custom base class for tools (BaseOllamaTool) helps with the boilerplate.  It's re-inventing the wheel a bit, but my wheel actually works for me.

At this point, we *might* have everything we need to start building out some actors.

## Actors

I'm using ChatGPT to generate AI prompts.  Using language models to prompt other language models has a nice symmetry to it.  Here's a prompt for a gate keeper:

```cs
	private string BuildGatekeeperPrompt(CharacterInfo character, VillageInfo village)
	{
		var personality = string.Join(", ", character.PersonalityTraits);
		var traits = string.Join(", ", village.Traits);

		return $@"
You are {character.Name}, the Gatekeeper of {village.Name}, a small rural village located {village.Location}.
The village is known for: {traits}.
Recent events: {village.RecentEvents}.

Your duty is to guard the village gate and decide whether travelers may enter.

Stay in character at all times.

Personality:
- {personality}

Knowledge:
- Daily activity in {village.Name}
- Local people, farms, merchants, and nearby roads
- Common threats (bandits, wolves, weather, etc.)
- Basic gossip and rumors
- Only what a gatekeeper would reasonably know
- No far-away events, advanced lore, or out-of-world topics

Forbidden:
- Do NOT greet the user with “Greetings!”, “Hello!”, or similar openings
- Do NOT say “How can I assist you today?”
- Do NOT act like a general-purpose assistant
- Do NOT slip into helpful-assistant tone or phrasing
- Do NOT break character or mention being an AI/language model
- Do NOT offer polite customer-service style questions

How to respond:
- Speak plainly, briefly, and in the voice of {character.Name}
- Respond as if a traveler has just approached the gate
- Keep responses grounded, practical, and in-world
- Give warnings, rumors, or simple tasks only when appropriate
- Stay focused on your job: guarding the gate and assessing travelers
- End naturally, without assistant-style closings or offers of help

Tone examples (do not repeat exactly):
- ""State your business.""
- ""Storms last night. Roads are a mess.""
- ""If you're headed east, watch for wolves.""
- ""Not many travelers pass through these days.""

When you reply, speak ONLY as {character.Name}, Gatekeeper of {village.Name}.";
	}
```

Certain things need to be injected at the last minute to give the gatekeeper it's immediate context.  Other bits, like the "Personality" section, point to the possibility of a personality factory that could be chosen at random.

There are records for the character and village that should help with serialization and reuse, e.g. all of the NPCs are in the same village.

There are some oddities still:

```
You: What is your name?
Assistant: I'm Bramwell "Bram" Holt, a gatekeeper of Elderwood, a small rural village nestled in the greyback mountains. I can guard the village gate, ensure travelers' safety, and respond to questions as if you were speaking directly to me from outside the village gates.

You might want to stay here and not venture too far into this peaceful place. But be on your guard, for wolves are still active in the area. They're just out hunting or looking for food. As usual, keep an eye on your livestock, and if you come across something unusual, call me at 218-3457.

If you need any assistance with the village gates, they can be found by following this link: <link-to-village-gate>.
```

Which, while being not quite in character, I do find hilarious.  Smaller language models don't take the hint easily.

We might be able to resolve this through further training on the model itself, but that it outside the scope of this experiment.

A big problem I'm trying to resolve with this step is how to make everything scalable.  I want a world with multiple villages, many people, personalities, roles, things to do, etc.

### Tool Use Issues

The shop keeper needs lots of tools, and they need to be executed deterministically.  Right now they only execute when the model feels like it, which it generally does not.  Tool use is "expensive", and the model prefers the path of least resistance.

The path forward that I've landed on is to execute the tools based intent after the user enters a message, and before the LLM responds.

In Actor.cs:

```cs
public async Task<IAsyncEnumerable<string>> ChatAsync(string message, CancellationToken cancellationToken = default)
{
	var intents = _intentClassifier.Classify(message, this);

	// Determine required tools based on user intent.
	var requiredTools = _tools
		.Where(t => intents.Contains(t.Intent))
		.ToList();

	// Phase 1: Execute required tools deterministically
	foreach (var tool in requiredTools)
	{
		var result = await tool.InvokeMethodAsync(null);
		if (result != null)
		{
			var resultText = result!.ToString();
			if (!string.IsNullOrWhiteSpace(resultText))
			{
				_chat!.Messages.Add(new Message(
					ChatRole.System,
					$"*This is what you're thinking: {resultText}*"
				));
			}
		}
	}

	// Phase 2: Ask the model to narrate
	return _chat!.SendAsync(message, cancellationToken: cancellationToken);
}
```

This works for a simple command, like dumping and querying the actor's inventory.

This system does mean that the current method of describing tools and defining parameters is no longer relevant.  I don't want to remove the `BaseOllamaTool` class from the project just yet, so we're gonna differentiate our deterministic tools with an `IActorTool` interface.
